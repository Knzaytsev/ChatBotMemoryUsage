{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7bcc238-cb60-4dda-952b-e858fae188f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hugchat\n",
      "  Downloading hugchat-0.0.6.2-py3-none-any.whl (18 kB)\n",
      "Collecting requests-toolbelt\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m860.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from hugchat) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests->hugchat) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->hugchat) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->hugchat) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->hugchat) (1.26.11)\n",
      "Installing collected packages: requests-toolbelt, hugchat\n",
      "Successfully installed hugchat-0.0.6.2 requests-toolbelt-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install hugchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741e373e-1d82-4242-a9ba-c7420ff5407c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "<empty message>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/88/77rz0tyn27s99tjty03mzhz80000gn/T/ipykernel_63194/265393316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   messages=[\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             return (\n\u001b[0;32m--> 624\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    688\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: <empty message>"
     ]
    }
   ],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "\n",
    "openai.api_key = 'test'\n",
    "\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee6bc7d-a401-4a28-b82c-a42444ea0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugchat import hugchat\n",
    "import json\n",
    "import re\n",
    "chatbot = hugchat.ChatBot(cookie_path=\"cookies.json\")  # or cookies=[...]\n",
    "# print(chatbot.chat(\"HI\"))\n",
    "\n",
    "# # Create a new conversation\n",
    "# id = chatbot.new_conversation()\n",
    "# chatbot.change_conversation(id)\n",
    "\n",
    "# # Get conversation list\n",
    "# conversation_list = chatbot.get_conversation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b6d86f-07dd-4e53-ab8c-bd8fd4ee46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vicuna_experiments/annotation_dataset.json', 'r') as f:\n",
    "    annotation_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb1e5903-5456-4661-b382-309aaba9db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vicuna_experiments/annotated_data.jsonl', 'r') as f:\n",
    "    annotated_data = f.readlines()\n",
    "    annotated_data = [json.loads(data) for data in annotated_data]\n",
    "    annotated_data.insert(83, {'id': 83, 'Answer': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c0f595-9a99-4ab8-9097-110e0c7dfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9468f7a3-c8ea-4c5e-9528-f6b8b7cc7183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3d9a82-7092-4c89-8e4f-64400f4ce9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [05:05<00:00, 11.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "answer_extractor = lambda x: [re.findall(r'Answer [AB]', answer) for answer in x]\n",
    "most_common = lambda x: max(set(x), key=x.count)\n",
    "\n",
    "PROMPT_QUESTION = '''\n",
    "Please choose the most relevant answer.\n",
    "The most relevant answer should follow the next rules:\n",
    "1. The answer should not contradict with context, facts and dialog.\n",
    "2. The answer should be natural.\n",
    "3. The answer should not be absurd or meaningless.\n",
    "Just write Answer A or Answer B.\n",
    "'''\n",
    "for sample in tqdm(annotation_data[len(answers):]):\n",
    "    prompt = '\\n'.join([sample['dialog'], 'Answer A:', sample['Answer A'], \n",
    "                        'Answer B:', sample['Answer B'], '\\n' + PROMPT_QUESTION])\n",
    "    \n",
    "    sample_answers = []\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            sample_answers.append(chatbot.chat(prompt))\n",
    "        except KeyboardInterrupt:\n",
    "            raise KeyboardInterrupt\n",
    "        except:\n",
    "            pass\n",
    "    sample_answers = answer_extractor(sample_answers)\n",
    "    sample_answers = [answer[0] for answer in sample_answers if answer]\n",
    "    answers.append(most_common(sample_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5eadfc-2bda-4237-8c1d-28d641f3ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_len = [len(data['dialog'][re.match(r'.*Dialog:\\n', data['dialog'], re.S).span()[1]:-2].split('\\n'))\n",
    "               for data in annotation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc5cd809-d475-4dc2-b07a-3695716ddb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bins = [0, 4, 8, 11]\n",
    "\n",
    "report = pd.DataFrame({'chatbot_answers': answers, \n",
    "                       'human_answers': [data['Answer'] for data in annotated_data],\n",
    "                       'session_len': session_len})\n",
    "report['bin'] = pd.cut(report['session_len'], bins)\n",
    "report['chatbot_context'] = report['chatbot_answers'] == [data['Contexted Answer'] for data in annotation_data]\n",
    "report['human_context'] = report['human_answers'] == [data['Contexted Answer'] for data in annotation_data]\n",
    "report = report[report['human_answers'].isin(['Answer A',  'Answer B'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e59e16f-39f0-46e3-81ba-0182fe0b19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv('huggingchat_answers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65fdc9ac-d5a5-4003-9513-e6630dd0d456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatbot_context</th>\n",
       "      <th>human_context</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0, 4]</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 8]</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 11]</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         chatbot_context  human_context\n",
       "bin                                    \n",
       "(0, 4]              0.58           0.60\n",
       "(4, 8]              0.49           0.63\n",
       "(8, 11]             0.45           0.52"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.groupby(['bin']).agg({'chatbot_context': 'sum', 'human_context': 'sum'}) \\\n",
    "        .div(report.groupby(['bin']).agg({'chatbot_context': 'count', 'human_context': 'count'})).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2eaf5382-313c-43d7-96ea-dbae62194da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641025641025641"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(report['chatbot_answers'] == report['human_answers']).sum() / report.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d28ff0af-177b-4eb4-ab98-b215bc713a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([annotation_data[i]['Contexted Answer'] == answer for i, answer in enumerate(answers)]) / len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f7a16dab-f6a7-468a-ae26-a17918e70064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: How are you and your cats doing today over in Alaska?\\nAssistant: Good thanks. How are you and your dad in Virginia? \\nHuman: We are doing pretty good thank you for asking. I hope you don't have any headaches today!\\n\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_data[0]['dialog'][re.match(r'.*Dialog:\\n', annotation_data[0]['dialog'], re.S).span()[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad09b8d3-49af-47b0-88b0-0a73fd4077c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Context:\\n1. The Assistant has vision problems and has called 911 in the past due to severe headaches.\\n2. The Assistant used to have six cats and is now married to a friend from high school.\\n3. The Human is from Virginia and is not currently married.\\n\\n Facts about Human:\\nHuman is not married and lives in Virginia.\\n\\nFacts about Assistant:\\nWorks from home\\nHas trouble seeing\\nHas 200 feet vision\\nGets bad headaches\\nHas called 911 before\\nUsed to have 6 cats\\nLives in Alaska\\nAssistant has vision problems and gets headaches.\\nAssistant is from Alaska and got married to a friend from high school.\\n\\n Dialog:\\nHuman: How are you and your cats doing today over in Alaska?\\nAssistant: Good thanks. How are you and your dad in Virginia? \\nHuman: We are doing pretty good thank you for asking. I hope you don't have any headaches today!\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_data[0]['dialog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12a54f90-56f4-487f-9618-c6529c3c1b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4765625"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([annotation_data[i]['Contexted Answer'] == answer[0] for i, answer in extracted_answers]) / len(extracted_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bec452ec-3839-40c7-aed8-3217ae9c0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "extracted_answers = [(i, re.findall(r'Answer [AB]', answer)) for i, answer in enumerate(answers)]\n",
    "extracted_answers = [answer for answer in extracted_answers if answer[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cc04203-6802-4093-8170-7ad0ce2d5ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4765625"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([annotation_data[i]['Contexted Answer'] == answer[0] for i, answer in extracted_answers]) / len(extracted_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "604d489f-6af8-480d-8eb6-cbe1705bbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chatbot.chat(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ecc8324-5af4-47ea-bc73-2b5fbddfec9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer B.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9900c9b-d721-45b6-a17d-fcdf3d2f367a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialog': \"Context:\\n1. The Assistant has vision problems and has called 911 in the past due to severe headaches.\\n2. The Assistant used to have six cats and is now married to a friend from high school.\\n3. The Human is from Virginia and is not currently married.\\n\\n Facts about Human:\\nHuman is not married and lives in Virginia.\\n\\nFacts about Assistant:\\nWorks from home\\nHas trouble seeing\\nHas 200 feet vision\\nGets bad headaches\\nHas called 911 before\\nUsed to have 6 cats\\nLives in Alaska\\nAssistant has vision problems and gets headaches.\\nAssistant is from Alaska and got married to a friend from high school.\\n\\n Dialog:\\nHuman: How are you and your cats doing today over in Alaska?\\nAssistant: Good thanks. How are you and your dad in Virginia? \\nHuman: We are doing pretty good thank you for asking. I hope you don't have any headaches today!\\n\",\n",
       " 'Answer A': 'Assistant: I am having a headache right now. My vision is messed up and I can hardly see. ',\n",
       " 'Answer B': \"Assistant: Thank you. I don't get as many headaches as I used to, but I still get them from time to time.\",\n",
       " 'Contexted Answer': 'Answer A'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e1909-23c1-4bb5-b953-9a706144c075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fec48d0e-7c40-4680-bc11-19a98da84b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers.json', 'r') as f:\n",
    "    answers = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7de71451-05c8-4515-a90f-2ad05f4d04a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 112851.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "answer_extractor = lambda x: [re.findall(r'Answer [AB]', answer) for answer in x]\n",
    "most_common = lambda x: max(set(x), key=x.count)\n",
    "\n",
    "extracted_answers = []\n",
    "for sample_answers in tqdm(answers):\n",
    "    sample_answers = answer_extractor(sample_answers)\n",
    "    sample_answers = [answer[0] for answer in sample_answers if answer]\n",
    "    extracted_answers.append(most_common(sample_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "108c8621-8206-4899-8d8a-52443bfe249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5266666666666666"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([annotation_data[i]['Contexted Answer'] == answer for i, answer in enumerate(extracted_answers)]) / len(extracted_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e97e70d4-f355-4551-925b-8a60f4096e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_annotated_data = [(i, data) for i, data in enumerate(annotated_data) if data['Answer'] in ['Answer A', 'Answer B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16cdb030-c24b-4845-b61b-51dc78534459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('huggingchat_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43389810-b381-4bb2-81f5-ed277658fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename({'chatbot_answers': 'open_assistant_answers', 'chatbot_context': 'open_assistant_context'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5afa6ad6-27ec-49a7-8167-6bc7062816f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['gpt_4_answers'] = [extracted_answers[i] for i, answer in correct_annotated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f406a88e-fbae-4f5e-a08a-a6ff8a9f6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['open_assistant_context'] = dataset['open_assistant_answers'] == [data['Contexted Answer'] for i, data in correct_annotated_data]\n",
    "dataset['human_context'] = dataset['human_answers'] == [data['Contexted Answer'] for i, data in correct_annotated_data]\n",
    "dataset['gpt_4_context'] = dataset['gpt_4_answers'] == [data['Contexted Answer'] for i, data in correct_annotated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66a1fd98-0330-4713-add9-30797282827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dialog_evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4b524cd-b15f-4cb3-b734-74204382d50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_annotated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
